from fastapi import FastAPI
from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore
from fastapi import FastAPI, File, UploadFile
from haystack.nodes import DensePassageRetriever
# reader model
from haystack.nodes import FARMReader 
from haystack.pipelines import ExtractiveQAPipeline
import uvicorn
import os
import shutil


# node_path = '/home/ec2-user/elasticsearch-7.17.0/data/nodes/'

# if os.path.exists(node_path):
#     for folder in os.listdir(node_path):
#         shutil.rmtree(os.path.join(node_path,folder))

pipeline = None
index = []

#Reader Object
reader = FARMReader(model_name_or_path='deepset/bert-base-cased-squad2')

# initialise api
app = FastAPI()

# To get the answer for query
@app.get('/dpr_query')
async def query(q):
    global pipeline
    res= pipeline.run(query=q,params={"Retriever": {"top_k": 10},'Reader':{'top_k':5}})
    ans=[x.to_dict() for x in res["answers"]]
    for i,obj in enumerate(ans):
        print(f'Answer {i+1} : {obj["answer"]}')
        
    return res

# To Input the data (external file)
@app.post('/write_documents')
async def write(file : UploadFile = File(...)):
    global pipeline, index

    #External File name
    file_location = file.filename
    idx = file_location.split('.')[0]
    index.append(idx)
    print(index)
    with open(file_location,'wb') as f:
        f.write(file.file.read())
    
    doc_store = ElasticsearchDocumentStore(
        host='localhost',
        username='', password='',
        index=idx
    )
    with open(file_location, 'r',encoding='utf-8') as f:
        data = f.read()

    data = data.split('\n')
    data_json = [
        {
            'content': paragraph,
            'meta': {
                'source': idx
            }
        } for paragraph in data
    ]
    if len(index) == 1:
        idx = index[0]
        print(idx)
    else:
        idx = index[-1]    
        doc_store.delete_index(index[0])

    doc_store.write_documents(data_json,index=idx)
    # print(doc_store.get_all_documents(index='test'))
   

    #Retriever
    retriever = DensePassageRetriever(
    document_store=doc_store,
    query_embedding_model='facebook/dpr-question_encoder-single-nq-base',
    passage_embedding_model='facebook/dpr-ctx_encoder-single-nq-base',
    # if gpu is available
    use_gpu=True, 
    embed_title=True)

    #Update Embeddings
    doc_store.update_embeddings(retriever=retriever,index=idx)

    #Creating Pipeline object
    pipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever)

    return 'Documents written successfully!'

if __name__ == '__main__':
    uvicorn.run(app,host='0.0.0.0',port=8080)



# import torch
# import gpt_2_simple as gpt2

# # Load the pre-trained GPT-2 model
# gpt2.download_gpt2(model_name='124M')  # Download the GPT-2 model if not already downloaded
# sess = gpt2.start_tf_sess()
# gpt2.load_gpt2(sess, model_name='124M')

# # Generate text using another model
# other_model_output = "This is the text generated by the other model."

# # Generate enhanced text using GPT-2
# gpt2_output = gpt2.generate(sess, prefix=other_model_output, length=100, temperature=0.7, return_as_list=True)[0]

# # Combine or refine the output
# final_output = gpt2_output  # In this example, we replace the other model's output with GPT-2's output

# # Print the final output
# print(final_output)

